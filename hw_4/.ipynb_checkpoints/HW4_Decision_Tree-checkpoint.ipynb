{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I. Implement a decision tree algorithm and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    \"\"\" Node class in the decision tree. \"\"\"\n",
    "    def __init__(self, T):\n",
    "        self.type = 'leaf' # Type of current node. Could be 'leaf' or 'branch' (at default: 'leaf').\n",
    "        self.left = None   # Left branch of the tree (for leaf node, it is None).\n",
    "        self.right = None  # Right branch of the tree (for leaf node, it is None).\n",
    "        self.dataset = T   # Dataset of current node, which is a tuple (X, Y). \n",
    "                           # X is the feature array and Y is the label vector.\n",
    "        \n",
    "    def set_as_leaf(self, common_class):\n",
    "        \"\"\" Set current node as leaf node. \"\"\"\n",
    "        self.type = 'leaf'\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.common_class = common_class\n",
    "    \n",
    "    def set_as_branch(self, left_node, right_node, split_rule):\n",
    "        \"\"\" Set current node as branch node. \"\"\"\n",
    "        self.type = 'branch'\n",
    "        self.left = left_node\n",
    "        self.right = right_node\n",
    "        # split_rule should be a tuple (j, t). \n",
    "        #   When x_j <= t, it goes to left branch. \n",
    "        #   When x_j > t, it goes to right branch.\n",
    "        self.split_rule = split_rule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for dataset.\n",
    "def get_dataset():\n",
    "    X = np.array(\n",
    "        [[1.0, 2.0],\n",
    "         [2.0, 2.0],\n",
    "         [3.0, 2.0],\n",
    "         [2.0, 3.0],\n",
    "         [1.0, 3.0]\n",
    "        ])\n",
    "    Y = np.array(\n",
    "        [1,\n",
    "         1,\n",
    "         0,\n",
    "         0,\n",
    "         0])\n",
    "    T = (X, Y) # The dataset T is a tuple of feature array X and label vector Y.\n",
    "    return T\n",
    "\n",
    "T = get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are required to implement the decision tree algorithm shown in the problem description of Q2 in HW4:\n",
    "\n",
    "<img src=\"./decision_tree_algorithm.png\" width=\"600\" />\n",
    "\n",
    "The **4 steps** are marked in comments of the following code. Please fill in the missing blanks (e.g. \"...\") in the TODOs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization.\n",
    "root_node = TreeNode(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (2058648195.py, line 123)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/w1/88wg69310jb3zpjbfphb72tc0000gn/T/ipykernel_29755/2058648195.py\"\u001b[0;36m, line \u001b[0;32m123\u001b[0m\n\u001b[0;31m    return Gini_Ti\u001b[0m\n\u001b[0m                  \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "# Procedure for current node.\n",
    "def build_decision_tree_procedure(node_cur, depth=0):\n",
    "    # Step 1. Check if all data points in T_cur are in the same class\n",
    "    #         - If it is true, set current node as a *leaf node* to predict the common class in T_cur, \n",
    "    #           and then terminate current procedure.\n",
    "    #         - If it is false, continue the procedure.\n",
    "    \n",
    "    T_cur = node_cur.dataset\n",
    "    X_cur, Y_cur = T_cur  # Get current feature array X_cur and label vector Y_cur.\n",
    "    if (Y_cur == 1).all():\n",
    "        print('    ' * depth + '+-> leaf node (predict 1).')\n",
    "        print('    ' * depth + '      Gini: {:.3f}'.format(Gini(T_cur)))\n",
    "        print('    ' * depth + '      samples: {}'.format(len(X_cur)))\n",
    "        node_cur.set_as_leaf(1)\n",
    "        return\n",
    "    elif (Y_cur == 0).all():\n",
    "        print('    ' * depth + '+-> leaf node (predict 0).')\n",
    "        print('    ' * depth + '      Gini: {:.3f}'.format(Gini(T_cur)))\n",
    "        print('    ' * depth + '      samples: {}'.format(len(X_cur)))\n",
    "        node_cur.set_as_leaf(0)\n",
    "        return\n",
    "\n",
    "    # Step 2. Traverse all possible splitting rules.\n",
    "    #         - We will traverse the rules over all feature dimensions j in {0, 1} and\n",
    "    #           thresholds t in X_cur[:, j] (i.e. all x_j in current feature array X_cur).\n",
    "\n",
    "    all_rules = []\n",
    "\n",
    "    \n",
    "    #### TODO 1 STARTS ###\n",
    "    # Please traverse the rules over all feature dimensions j in {0, 1} and \n",
    "    #   thresholds t in X_cur[:, j] (i.e. all x_j in current feature array X_cur), \n",
    "    #   and save all rules in all_rules variable.\n",
    "    # The all_rules variable should be a list of tuples such as [(0, 1.0), (0, 2.0), ... ]\n",
    "    \n",
    "    for j in range(np.size(X_cur, axis= 1)):\n",
    "        for t in set(X_cur):\n",
    "            all_rules.append((j,t))\n",
    "    #### TODO 1 ENDS ###\n",
    "    \n",
    "    print('All rules:', all_rules)\n",
    "    \n",
    "'''\n",
    "    # Step 3. Decide the best splitting rule.\n",
    "    best_rule = (_, _)\n",
    "    best_weighted_sum = 1.0\n",
    "    for (j, t) in all_rules:\n",
    "       \n",
    "        #### TODO 2 STARTS ###\n",
    "        # For each splitting rule (j, t), we use it to split the dataset T_cur into T1 and T2.\n",
    "        # Hint: You may refer to Step 4 to understand how to set inds1, X1, Y1, len_T1 and inds2, X2, Y2, len_T2.\n",
    "        \n",
    "        # - Create subset T1.\n",
    "        inds1 = ...                 # Indices vector for those data points with x_j <= t.\n",
    "        X1 = ...                    # Feature array with inds1 in X_cur.\n",
    "        Y1 = ...                    # Label vector with inds1 in Y_cur.\n",
    "        T1 = (X1, Y1)               # Subset T1 contains feature array and label vector.\n",
    "        len_T1 = ...                # Size of subset T1.\n",
    "        # - Create subset T2.\n",
    "        inds2 = ...                 # Indices vector for those data points with x_j > t.\n",
    "        X2 = ...                    # Feature array with inds2 in X_cur.\n",
    "        Y2 = ...                    # Label vector with inds2 in Y_cur.\n",
    "        T2 = (X2, Y2)               # Subset T2 contains feature array and label vector.\n",
    "        len_T2 = ...                # Size of subset T2.\n",
    "        #### TODO 2 ENDS ###\n",
    "        \n",
    "        # Calculate weighted sum and try to find the best one.\n",
    "        weighted_sum = (len_T1*Gini(T1) + len_T2*Gini(T2)) / (len_T1 + len_T2)\n",
    "        # print('Rule:', (j, t), 'len_T1, len_T2:', len_T1, len_T2, 'weighted_sum:', weighted_sum)  # Code for debugging.\n",
    "        if weighted_sum < best_weighted_sum:\n",
    "            \n",
    "            #### TODO 3 STARTS ####\n",
    "            # Update the best rule and best weighted sum with current ones.\n",
    "            \n",
    "            best_rule = ...\n",
    "            best_weighted_sum = ...\n",
    "            #### TODO 3 ENDS ####\n",
    "    \n",
    "    # Step 4. - We split the dataset T_cur into two subsets best_T1, best_T2 following \n",
    "    #               the best splitting rule (best_j, best_t). \n",
    "    #         - Then we set current node as a *branch* node and create child nodes with \n",
    "    #               the subsets best_T1, best_T2 respectively. \n",
    "    #         - For each child node, start from *Step 1* again recursively.\n",
    "    \n",
    "    best_j, best_t = best_rule\n",
    "    # - Create subset best_T1 and corresponding child node.\n",
    "    best_inds1 = X_cur[:,best_j] <= best_t\n",
    "    best_X1 = X_cur[best_inds1]\n",
    "    best_Y1 = Y_cur[best_inds1]\n",
    "    best_T1 = (best_X1, best_Y1)\n",
    "    node1 = TreeNode(best_T1)\n",
    "    # - Create subset best_T2 and corresponding child node.\n",
    "    best_inds2 = X_cur[:,best_j] > best_t\n",
    "    best_X2 = X_cur[best_inds2]\n",
    "    best_Y2 = Y_cur[best_inds2]\n",
    "    best_T2 = (best_X2, best_Y2)\n",
    "    node2 = TreeNode(best_T2)\n",
    "    # - Set current node as branch node and create child nodes.\n",
    "    node_cur.set_as_branch(left_node=node1, right_node=node2, split_rule=best_rule)\n",
    "    print('    ' * depth + '+-> branch node')\n",
    "    print('    ' * depth + '      Gini: {:.3f}'.format(Gini(T_cur)))\n",
    "    print('    ' * depth + '      samples: {}'.format(len(X_cur)))\n",
    "    # - For each child node, start from Step 1 again recursively.\n",
    "    print('    ' * (depth + 1) + '|-> left branch: x_{} <= {} (with {} data point(s)).'.format(best_j, best_t, len(best_X1)))\n",
    "    build_decision_tree_procedure(node1, depth+1) # Note: The depth is only used for logging.\n",
    "    print('    ' * (depth + 1) + '|-> right branch: x_{} > {} (with {} data point(s)).'.format(best_j, best_t, len(best_X2)))\n",
    "    build_decision_tree_procedure(node2, depth+1)'''\n",
    "        \n",
    "def Gini(Ti):\n",
    "    \"\"\" Calculate the Gini index given dataset Ti. \"\"\"\n",
    "    Xi, Yi = Ti        # Get the feature array Xi and label vector Yi.\n",
    "    if len(Yi) == 0:   # If the dataset Ti is empty, it simply returns 0.\n",
    "        return 0\n",
    "    \n",
    "    #### TODO 4 STARTS ####\n",
    "    # Implement the Gini index function.\n",
    "    \n",
    "    P_Y1 = ...         # Estimate probability P(Y=1) in Yi\n",
    "    P_Y0 = ...         # Estimate probability P(Y=0) in Yi\n",
    "    Gini_Ti = ...      # Calculate Gini index: Gini_Ti = 1 - P(Y=1)^2 - P(Y=0)^2\n",
    "    #### TODO 4 ENDS ####\n",
    "    \n",
    "    return Gini_Ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you finish the above code blank filling, you can use the following code to build the decision tree. The following code also shows the structure of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All rules: [(0, 1), (0, 2), (0, 3), (1, 1), (1, 2), (1, 3)]\n"
     ]
    }
   ],
   "source": [
    "# Build the decision tree.\n",
    "build_decision_tree_procedure(root_node)\n",
    "\n",
    "# If your code is correct, you should output:\n",
    "#\n",
    "# +-> branch node\n",
    "#       Gini: 0.480\n",
    "#       samples: 5\n",
    "#     |-> left branch: x_1 <= 2.0 (with 3 data point(s)).\n",
    "#     +-> branch node\n",
    "#           Gini: 0.444\n",
    "#           samples: 3\n",
    "#     .....\n",
    "#\n",
    "# You can also use the sklearn results to validate your decision tree\n",
    "# (the threshold could be slightly different but the structure of the tree should be the same)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the obtained decision tree, you can predict the class of new feature vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_predict(node_cur, x):\n",
    "    if node_cur.type == 'leaf':\n",
    "        return node_cur.common_class\n",
    "    else:\n",
    "        j, t = node_cur.split_rule\n",
    "        if x[j] <= t:\n",
    "            return decision_tree_predict(node_cur.left, x)\n",
    "        else:\n",
    "            return decision_tree_predict(node_cur.right, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [(2,1), (3,1), (3,3)]:\n",
    "    y_pred = decision_tree_predict(root_node, x)\n",
    "    print('Prediction of {} is {}'.format(x, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II. Use Scikit-learn to build the tree and make predictions.\n",
    "\n",
    "The following code uses Scikit-learn to build the decision tree. You can use it to check if your previous implementation is correct or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart\n",
    "from sklearn import tree\n",
    "X, Y = T\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code illustrates the obtained decision tree. It should have same structure and similar rules compared with the tree in your own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the tree.\n",
    "tree.plot_tree(clf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code makes the predictions using the obtained decision tree. It should have identical results as the ones for your own implementaion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class.\n",
    "for x in [(2,1), (3,1), (3,3)]:\n",
    "    y_pred = clf.predict(np.array([x]))[0]\n",
    "    print('Prediction of {} is {}'.format(x, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
