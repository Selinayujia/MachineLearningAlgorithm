{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5: Image classification with Convolutional Neural Networks (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, you'll build simple convolutional neural networks using Keras for image classification tasks. The goal is to get you familiar with the steps of working with deep learning models, namely, preprocessing dataset, defining models, train/test models and quantatively comparing performances.\n",
    "Make sure this notebook is launched in an environment with Numpy, Tensorflow, matplotlib and Keras installed. Refer\n",
    "to: https://www.tutorialspoint.com/keras/keras_installation.htm if you need help with creating a virtual environment with all required dependencies. \n",
    "\n",
    "Furthermore, you can refer to the official Keras website for detailed documentations about different neural network layers (https://keras.io/api/layers/) and other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Sample code (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in class, we first download the MNIST dataset and get the train/test sets. We then process the data to be ready for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_dataset(trainX, trainY, testX, testY):\n",
    "    # reshape features and normalize\n",
    "    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "    testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    "    trainX = trainX.astype('float32')\n",
    "    testX = testX.astype('float32')\n",
    "    trainX = trainX / 255.0\n",
    "    testX = testX / 255.0\n",
    "    # converting labels to one-hot encoding\n",
    "    trainY = np_utils.to_categorical(trainY)\n",
    "    testY = np_utils.to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY\n",
    "trainX, trainY, testX, testY = process_dataset(trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the model. Similar to in-class demo, this model has 1 convolution layer with 32 filters, followed by one \n",
    " 2-by-2 MaxPooling layer. The output from MaxPooling layer is then flattened and goes through two linear layers, with 100 and 10 hidden\n",
    "units respectively. We use Stochastic Gradient Descent as our optimizer, and we can adjust its learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can train and evaulate the specified model. Here we're using the test set as the validation set for simplicity. \n",
    "However, to be more rigorous we often split the training dataset into train/validation sets and tune the hyperparameters using \n",
    "only the training dataset, and we test the model on the test set after figuring out the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define a model with lr=0.01\n",
    "model = define_model(0.01)\n",
    "history = model.fit(trainX, trainY, batch_size=32, epochs=10, validation_data=(testX, testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is completed, we can plot the train/validation losses and train/validation accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss\n",
    "fig = plt.figure()\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history.history['loss'], color='blue', label='train')\n",
    "plt.plot(history.history['val_loss'], color='orange', label='val')\n",
    "plt.legend(('train','val'))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "# plot accuracy\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "plt.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "plt.legend(('train','val'))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (5 points):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe in the above plots? What do you think might be the reason?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Vary learning rates (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lecture that we update the weights of the neural network by first calculate the gradients with backpropagation from the loss $L$, then update the weights by $$ w = w - \\eta*\\frac{\\partial L}{\\partial w}$$\n",
    "Here, $\\eta$ is the learning rate and decides the step size of updates. Previously we used $\\eta=0.01$. We want to see the effect of learning rate on the training process, therefore we would like to try two other choices of $\\eta$. (1) $\\eta=1$ (2) $\\eta=$1e-5 (0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### TODO 1 STARTS ###\n",
    "model_eta_large = ...\n",
    "history_eta_large = ...\n",
    "#### TODO 1 ENDS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### TODO 2 STARTS ###\n",
    "model_eta_small = ...\n",
    "history_eta_small = ...\n",
    "#### TODO 2 ENDS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the training accuracy of the two above models with the training accuracy of the model in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['accuracy'], color='blue')\n",
    "plt.plot(history_eta_small.history['accuracy'], color='orange')\n",
    "plt.plot(history_eta_large.history['accuracy'], color='red')\n",
    "plt.legend(('lr=0.01','lr=0.00001','lr=1'))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (5 points):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe by looking at the training accuracies above? Does the two other models with small and large learning\n",
    "rates seem to be learning? What do you think might be the reason? (optional) Can you find a better learning rate than the baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Adding momentum (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now we have tried various learning rates with SGD. There are various ways to make SGD behave more intelligently, \n",
    "one of which is momentum. Intuitively, when SGD tries to descend down a valley (an analogy for the case \n",
    "where the gradient of one dimension is larger than gradient of another dimension), SGD might bounce between the walls of the valley instead of descending along the valley. This makes SGD converge slower or even stuck. Momentum works by dampening the oscillations of SGD and encourages it to follow a smoother path. Formally, SGD with momentum update weights by the following way:\n",
    "\n",
    "$$z^{k+1} = \\beta z^{k} + \\frac{\\partial L}{\\partial w^k}$$\n",
    "$$w^{k+1} = w^{k} - \\eta*z^{k+1}$$\n",
    "\n",
    "Here $\\beta$ is the momentum and is between 0 and 1. The official documentation of SGD details how to specify momentum (https://keras.io/api/optimizers/sgd/). If you want to learn more about momentum, this post might be helpful: https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please define a model with learning rate 0.01 and momentum 0.9, then compare it to the baseline in part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model_with_momentum(learning_rate,momentum):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    #### TODO 3 STARTS ###\n",
    "    opt = ...\n",
    "    #### TODO 3 ENDS ###\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TODO 4 STARTS ###\n",
    "model_momentum = ...\n",
    "history_momentum = ...\n",
    "#### TODO 4 ENDS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['accuracy'], color='blue')\n",
    "plt.plot(history_momentum.history['accuracy'], color='orange')\n",
    "plt.legend(('w/o momentum','w/ momentum'))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (5 points):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe in the plot? Does momentum improves training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Adding convolution layers (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase model capacity (the ability to fit more complex dataset), one way is to adding layers to the model. In part \n",
    "1, the model given to you has the following layers before the final 2 dense layers:\n",
    "    \n",
    "(1) 2D convolution with 32 filters of size 3-by-3, stride 1-by-1, 'valid' padding and relu activations\n",
    "\n",
    "(2) 2-by-2 Max Pooling layer \n",
    "\n",
    "(2) Flatten layer\n",
    "\n",
    "In the function below, please implement a model with the following layers (in this order):\n",
    "\n",
    "(1) 2D convolution with 32 filters of size 3-by-3, stride 1-by-1, 'valid' padding and relu activations\n",
    "\n",
    "(2) 2-by-2 Max Pooling layer \n",
    "\n",
    "(1) 2D convolution with 64 filters of size 3-by-3, stride 1-by-1, 'valid' padding and relu activations\n",
    "\n",
    "(2) 2-by-2 Max Pooling layer \n",
    "\n",
    "(2) Flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model_2_conv(learning_rate):\n",
    "    model = Sequential()\n",
    "    #### TODO 5 STARTS ###\n",
    "    # adding layers here\n",
    "    \n",
    "    #### TODO 5 ENDS ###\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and train\n",
    "#### TODO 6 STARTS ###\n",
    "model_2_layer = ...\n",
    "history_2_layer = ...\n",
    "#### TODO 6 ENDS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['accuracy'], color='blue')\n",
    "plt.plot(history_2_layer.history['accuracy'], color='orange')\n",
    "plt.legend(('1 conv layer','2 conv layers'))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (5 points):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe in the plot? Does adding a covolutional layer improves training set accuracy? What might be\n",
    "the reason to the improvement if there are any?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
